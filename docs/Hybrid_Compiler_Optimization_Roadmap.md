# Hybrid Compiler Optimization Roadmap

This roadmap outlines the optimization strategy for transforming the single-pass Orus compiler into a hybrid compiler, addressing the estimated 1.1–4.6x slowdown introduced by pre- and post-passes. The optimizations aim to minimize compilation time, reduce memory usage, and improve generated bytecode efficiency, while preserving type safety and supporting complex features like closures, loop optimizations, and long jumps. The roadmap is divided into phases with specific tasks, timelines, and expected outcomes.

## Phase 1: Pre-Pass Optimization (Weeks 1–2)
**Goal**: Reduce the overhead of the pre-pass (estimated at ~25 ms for 10,000 nodes) by minimizing AST traversals and optimizing metadata collection.

### Tasks

#### Combine Pre-Pass Traversals
- **Task**: Merge `preprocess_functions` and `preprocess_loops` into a single AST traversal to collect `FunctionTable` and `LoopTable` data simultaneously.
- **Implementation**: Update `preprocess_node` to handle both `NODE_FUNCTION` and `NODE_FOR_RANGE`/`NODE_FOR_ITER` nodes in one pass, reducing traversal time from O(2n) to O(n).
- **Example**:
```c
void preprocess_node(ASTNode* node, Compiler* compiler) {
    if (node->type == NODE_PROGRAM || node->type == NODE_BLOCK) {
        for (int i = 0; i < node->program.count; i++) {
            preprocess_node(node->program.declarations[i], compiler);
        }
    } else if (node->type == NODE_FUNCTION) {
        // Add to FunctionTable
        FunctionEntry entry = {
            .name = node->function.name,
            .paramCount = node->function.paramCount,
            .returnType = node->function.returnType ? getExprType(node->function.returnType, compiler) : getPrimitiveType(TYPE_VOID),
            .upvalueCount = 0,
            .functionIdx = vm.functionCount++,
            .scopeDepth = compiler->scopeDepth
        };
        add_to_function_table(&compiler->functionTable, entry);
        count_upvalues(node->function.body, compiler, entry.functionIdx);
    } else if (node->type == NODE_FOR_RANGE || node->type == NODE_FOR_ITER) {
        // Add to LoopTable
        LoopEntry entry = {
            .varName = node->forRange.varName ? node->forRange.varName : node->forIter.varName,
            .varType = node->forRange.start ? getExprType(node->forRange.start, compiler) : getPrimitiveType(TYPE_ANY),
            .scopeDepth = compiler->scopeDepth + 1,
            .isRange = (node->type == NODE_FOR_RANGE),
            .invariantCount = 0,
            .invariants = NULL,
            .label = node->forRange.label ? node->forRange.label : node->forIter.label
        };
        analyze_invariants(node->forRange.body ? node->forRange.body : node->forIter.body, &entry.invariantCount, &entry.invariants, compiler);
        add_to_loop_table(&compiler->loopTable, entry);
    }
}
```
- **Expected Impact**: Reduce pre-pass time by ~50% (e.g., from 25 ms to ~12.5 ms for 10,000 nodes).

#### Cache Type Inference Results
- **Task**: Store results of `getExprType` in a hash table (`TypeCache`) to avoid redundant type computations across passes.
- **Implementation**: Add `TypeCache` to `Compiler` struct, mapping `ASTNode*` pointers to `Type*` results.
- **Example**:
```c
typedef struct {
    ASTNode* node;
    Type* type;
} TypeCacheEntry;

typedef struct {
    TypeCacheEntry* entries;
    int count;
    int capacity;
} TypeCache;

Type* getExprType(ASTNode* node, Compiler* compiler) {
    // Check cache
    for (int i = 0; i < compiler->typeCache.count; i++) {
        if (compiler->typeCache.entries[i].node == node) {
            return compiler->typeCache.entries[i].type;
        }
    }
    // Compute type (existing logic)
    Type* type = compute_expr_type(node, compiler);
    // Store in cache
    if (compiler->typeCache.count >= compiler->typeCache.capacity) {
        int oldCapacity = compiler->typeCache.capacity;
        compiler->typeCache.capacity = GROW_CAPACITY(oldCapacity);
        compiler->typeCache.entries = GROW_ARRAY(TypeCacheEntry, compiler->typeCache.entries, oldCapacity, compiler->typeCache.capacity);
    }
    compiler->typeCache.entries[compiler->typeCache.count++] = (TypeCacheEntry){node, type};
    return type;
}
```
- **Expected Impact**: Reduce type inference overhead by ~20–30% for expressions reused across passes (e.g., loop bounds, function parameters).

#### Selective Pre-Pass Processing
- **Task**: Skip nodes irrelevant to `FunctionTable` or `LoopTable` (e.g., `NODE_LITERAL`, `NODE_VAR_DECL`) during pre-pass.
- **Implementation**: Restrict `preprocess_node` to process only `NODE_FUNCTION`, `NODE_FOR_RANGE`, `NODE_FOR_ITER`, `NODE_BLOCK`, and `NODE_PROGRAM`.
- **Expected Impact**: Reduce pre-pass node visits by ~30–50%, depending on program structure (e.g., programs with many literals).

### Metrics
- **Target**: Reduce pre-pass time to ~10–15 ms for 10,000 nodes (from 25 ms).
- **Measurement**: Profile compilation time using a benchmark suite of Orus programs with varying function and loop counts.
- **Timeline**: Complete by Week 2.

## Phase 2: Main Pass Optimization (Weeks 3–4)
**Goal**: Optimize the main compilation pass (estimated at ~10.4 ms for 10,000 nodes) by reducing table lookups and invariant hoisting overhead.

### Tasks

#### Efficient Table Lookups
- **Task**: Replace linear searches in `FunctionTable` and `LoopTable` with hash tables for O(1) access.
- **Implementation**: Use existing `SymbolTable` infrastructure or a new hash table for `FunctionTable` and `LoopTable`.
- **Example**:
```c
typedef struct {
    HashTable functions; // Maps function name to FunctionEntry
    HashTable loops;     // Maps loop variable name + scope to LoopEntry
} CompilerTables;

void initCompilerTables(CompilerTables* tables) {
    hash_table_init(&tables->functions);
    hash_table_init(&tables->loops);
}

FunctionEntry* find_function(Compiler* compiler, const char* name, int scopeDepth) {
    char key[256];
    snprintf(key, sizeof(key), "%s:%d", name, scopeDepth);
    return hash_table_get(&compiler->tables.functions, key);
}
```
- **Expected Impact**: Reduce lookup time from O(f) or O(l) to O(1), saving ~0.2 ms for 100 functions/loops.

#### Optimize Invariant Hoisting
- **Task**: Limit LICM to expressions with high computational cost (e.g., binary operations, function calls) to avoid hoisting trivial expressions.
- **Implementation**: Add a cost heuristic in `analyze_invariants` to prioritize expressions.
- **Example**:
```c
bool is_high_cost_expression(ASTNode* node) {
    return node->type == NODE_BINARY || node->type == NODE_CALL || node->type == NODE_CAST;
}

void analyze_invariants(ASTNode* node, int* invariantCount, ASTNode*** invariants, Compiler* compiler) {
    if (node->type == NODE_BLOCK) {
        for (int i = 0; i < node->block.count; i++) {
            analyze_invariants(node->block.statements[i], invariantCount, invariants, compiler);
        }
    } else if (is_high_cost_expression(node) && !depends_on_loop_var(node, compiler)) {
        *invariants = GROW_ARRAY(ASTNode*, *invariants, *invariantCount, *invariantCount + 1);
        (*invariants)[(*invariantCount)++] = node;
    }
}
```
- **Expected Impact**: Reduce invariant hoisting overhead by ~50% (e.g., from 0.2 ms to 0.1 ms for 200 invariants).

#### Register Allocation Optimization
- **Task**: Reuse registers more aggressively by tracking live ranges within the main pass.
- **Implementation**: Add a simple live range analysis during `allocateRegister` to reuse freed registers.
- **Example**:
```c
uint8_t allocateRegister(Compiler* compiler) {
    for (int i = 0; i < compiler->maxRegisters; i++) {
        if (!compiler->registerInUse[i]) {
            compiler->registerInUse[i] = true;
            return i;
        }
    }
    uint8_t r = compiler->nextRegister++;
    if (r > compiler->maxRegisters) compiler->maxRegisters = r;
    compiler->registerInUse[r] = true;
    return r;
}

void freeRegister(Compiler* compiler, uint8_t reg) {
    compiler->registerInUse[reg] = false;
    if (reg + 1 == compiler->nextRegister && compiler->nextRegister > 0) {
        compiler->nextRegister--;
    }
}
```
- **Expected Impact**: Reduce register pressure, potentially saving ~0.1–0.2 ms for programs with high register usage.

### Metrics
- **Target**: Reduce main pass time to ~8–9 ms for 10,000 nodes (from 10.4 ms).
- **Measurement**: Benchmark register allocation and table lookup performance across test programs.
- **Timeline**: Complete by Week 4.

## Phase 3: Post-Pass Optimization (Weeks 5–6)
**Goal**: Minimize post-pass overhead (estimated at ~10.5 ms for 10,000 nodes) by optimizing jump/call patching and peephole optimizations.

### Tasks

#### Batch Jump and Call Patching
- **Task**: Combine `patchCalls` and `patchLongJumps` into a single bytecode scan to update all jump and call offsets.
- **Implementation**: Use a unified `PendingPatches` table to store both call and jump offsets.
- **Example**:
```c
typedef enum { PATCH_CALL, PATCH_JUMP } PatchType;

typedef struct {
    int offset;
    PatchType type;
    int targetIdx; // Function index for calls, jump target for jumps
} PatchEntry;

void patchBytecode(Compiler* compiler) {
    for (int i = 0; i < compiler->pendingPatches.count; i++) {
        PatchEntry* entry = &compiler->pendingPatches.entries[i];
        int offset = entry->offset;
        int jump = compiler->chunk->count - offset - 4;
        compiler->chunk->code[offset] = (jump >> 24) & 0xFF;
        compiler->chunk->code[offset + 1] = (jump >> 16) & 0xFF;
        compiler->chunk->code[offset + 2] = (jump >> 8) & 0xFF;
        compiler->chunk->code[offset + 3] = jump & 0xFF;
    }
    free_pending_patches(&compiler->pendingPatches);
}
```
- **Expected Impact**: Reduce patching time from ~0.5 ms to ~0.3 ms for 500 calls/jumps.

#### Selective Peephole Optimization
- **Task**: Apply peephole optimizations (e.g., replacing MUL with SHIFT for power-of-2 constants) only when an optimization flag is enabled.
- **Implementation**: Add a `--optimize` flag to enable/disable `peephole_optimize`.
- **Example**:
```c
void peephole_optimize(Chunk* chunk, bool optimizeEnabled) {
    if (!optimizeEnabled) return;
    for (int i = 0; i < chunk->count - 3; i++) {
        if (chunk->code[i] == OP_MUL_I32_R && is_power_of_2_constant(chunk, i + 3)) {
            chunk->code[i] = OP_SHIFT_LEFT_I32_R;
        }
    }
}
```
- **Expected Impact**: Allow zero post-pass time for non-optimized builds, saving ~10 ms when optimizations are disabled.

#### Incremental Bytecode Analysis
- **Task**: Perform peephole optimizations during the main pass for common patterns (e.g., consecutive MOVE instructions) to reduce post-pass work.
- **Implementation**: Check for optimizable patterns in `emitByte` and `emitBytes`.
- **Example**:
```c
void emitByte(Compiler* compiler, uint8_t byte) {
    if (byte == OP_MOVE && compiler->chunk->count >= 2 &&
        compiler->chunk->code[compiler->chunk->count - 1] == OP_MOVE) {
        // Coalesce consecutive MOVE instructions if possible
        return;
    }
    writeChunk(compiler->chunk, byte, compiler->currentLine, compiler->currentColumn);
}
```
- **Expected Impact**: Reduce post-pass optimization time by ~30% (e.g., from 10 ms to 7 ms).

### Metrics
- **Target**: Reduce post-pass time to ~5–7 ms for 10,000 nodes (from 10.5 ms) or ~0 ms for non-optimized builds.
- **Measurement**: Profile patching and optimization times with and without `--optimize` flag.
- **Timeline**: Complete by Week 6.

## Phase 4: Memory Optimization (Weeks 7–8)
**Goal**: Reduce memory usage of new data structures (`FunctionTable`, `LoopTable`, `TypeCache`) to minimize overhead.

### Tasks

#### Dynamic Memory Management
- **Task**: Use dynamic resizing for `FunctionTable`, `LoopTable`, and `TypeCache` to allocate only necessary memory.
- **Implementation**: Already using `GROw_ARRAY` in the plan; ensure initial capacities are small (e.g., 8 entries).
- **Expected Impact**: Reduce memory usage by ~50% for small programs (e.g., from 10 KB to 5 KB for 100 functions/loops).

#### Reuse Symbol Table
- **Task**: Store function and loop metadata in the existing `SymbolTable` instead of separate tables to reduce memory overhead.
- **Implementation**: Extend `SymbolEntry` to include function/loop metadata.
- **Example**:
```c
typedef struct {
    char* name;
    int index; // Positive for globals, negative for locals/loops, -2000+ for upvalues
    int scopeDepth;
    bool isFunction;
    FunctionEntry functionData; // Only if isFunction
    LoopEntry loopData;        // Only if is loop
} SymbolEntry;
```
- **Expected Impact**: Eliminate `FunctionTable` and `LoopTable`, saving ~2–4 KB for typical programs.

#### Clear Caches
- **Task**: Free `TypeCache` after the main pass to reduce memory usage during post-pass.
- **Implementation**: Add `freeTypeCache` call in `compile`.
- **Expected Impact**: Reduce peak memory usage by ~1–2 KB for large programs.

### Metrics
- **Target**: Reduce memory usage to ~5–10 KB for 100 functions/loops (from ~10–20 KB).
- **Measurement**: Use memory profiling tools (e.g., Valgrind) to measure peak memory usage.
- **Timeline**: Complete by Week 8.

## Phase 5: Long-Term Scalability (Weeks 9–12)
**Goal**: Ensure the compiler scales efficiently for large programs and supports future optimizations.

### Tasks

#### Incremental Compilation
- **Task**: Support recompiling only changed functions or modules to reduce compilation time for large codebases.
- **Implementation**: Add a dependency graph to track function and variable dependencies, recompile only affected nodes.
- **Expected Impact**: Reduce compilation time by ~50–90% for incremental changes (e.g., from 100 ms to 10–50 ms for 100,000 nodes).

#### Parallel Pre-Pass
- **Task**: Parallel咯

System: Parallelize the pre-pass for large ASTs by processing independent program branches (e.g., top-level declarations) concurrently.
- **Implementation**: Use a thread pool to process `NODE_PROGRAM` subtrees.
- **Expected Impact**: Reduce pre-pass time by ~30–50% on multi-core systems (e.g., from 100 ms to 50–70 ms for 100,000 nodes).

#### Advanced Optimizations
- **Task**: Implement additional optimizations like dead code elimination beyond constant folding, inlining small functions, and loop fusion.
- **Implementation**: Add an optimization pass in the post-pass for inlining and loop fusion.
- **Expected Impact**: Improve runtime performance by ~10–20% for optimized programs, with minimal compilation time increase (~2–3 ms).

### Metrics
- **Target**: Support programs with 100,000+ nodes with compilation times <500 ms and memory usage <100 KB.
- **Measurement**: Benchmark with large Orus programs and synthetic test cases.
- **Timeline**: Complete by Week 12.

## Expected Outcomes
- **Compilation Time**: Reduce hybrid compiler time from ~45.9 ms to ~25–30 ms for 10,000 nodes (2.5–3x slowdown vs. single-pass ~10 ms), with ~0 ms post-pass for non-optimized builds.
- **Memory Usage**: Reduce peak memory usage to ~5–10 KB for typical programs (100 functions/loops).
- **Runtime Performance**: Improve bytecode efficiency by 10–20% through LICM, peephole optimizations, and inlining.
- **Scalability**: Support large programs (100,000+ nodes) with <500 ms compilation time and incremental compilation for changes.

## Testing Plan
- **Benchmark Suite**: Create Orus programs with varying sizes (1,000 to 100,000 nodes) and complexities (e.g., nested functions, loops, closures).
- **Profiling**: Use tools like `gprof` and `Valgrind` to measure compilation time and memory usage.
- **Test Cases**:
```plaintext
fun fib(n) -> i32:
    if n <= 1 return n
    return fib(n-1) + fib(n-2)

for i in 1..100:
    var x = fib(i)
    print x;
```
- **Validation**: Ensure type safety, correct bytecode, and runtime performance match or exceed the single-pass compiler.

## Risks and Mitigations
- **Risk**: Increased code complexity from new data structures and passes.
  - **Mitigation**: Modularize code with clearA interfaces for `preprocess_node`, `patchBytecode`, etc.
- **Risk**: Optimization overhead outweighing benefits for small programs.
  - **Mitigation**: Enable optimizations via a flag, defaulting to fast compilation.
- **Risk**: Bugs in incremental compilation or parallel pre-pass.
  - **Mitigation**: Extensive testing with edge cases and regression suites.

## Conclusion
This roadmap optimizes the hybrid compiler to achieve a ~2–3x slowdown (from ~10 ms to ~25–30 ms for 10,000 nodes) compared to the single-pass compiler, while supporting complex features and improving runtime performance. By optimizing pre-pass traversals, main pass lookups, post-pass patching, and memory usage, the compiler will remain fast and scalable. Future work on incremental compilation and parallelization will ensure efficiency for large codebases.